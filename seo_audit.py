#!/usr/bin/env python3
"""
Ferramenta de Auditoria SEO Completa
Analisa uma URL e gera relat√≥rio detalhado de otimiza√ß√µes
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import time
import re
from collections import Counter
from datetime import datetime


class SEOAuditor:
    def __init__(self, url, keyword):
        self.url = url
        self.keyword = keyword.lower()
        self.domain = urlparse(url).netloc
        self.soup = None
        self.response = None
        self.ttfb = 0
        
        # Resultados da an√°lise
        self.critical_issues = []
        self.opportunities = []
        self.performance_data = {}
        self.action_plan = []
        
        # User-Agent para simular navegador real
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
    
    def fetch_page(self):
        """Busca a p√°gina e mede o tempo de resposta"""
        try:
            print(f"\nüîç Analisando: {self.url}")
            print("‚è≥ Aguarde, fazendo requisi√ß√£o...")
            
            start_time = time.time()
            self.response = requests.get(self.url, headers=self.headers, timeout=10)
            self.ttfb = time.time() - start_time
            
            if self.response.status_code != 200:
                self.critical_issues.append(
                    f"‚ùå Status HTTP {self.response.status_code} - P√°gina n√£o acess√≠vel"
                )
                return False
            
            self.soup = BeautifulSoup(self.response.content, 'html.parser')
            return True
            
        except requests.exceptions.Timeout:
            self.critical_issues.append("‚ùå Timeout - Servidor demorou mais de 10s para responder")
            return False
        except requests.exceptions.ConnectionError:
            self.critical_issues.append("‚ùå Erro de conex√£o - N√£o foi poss√≠vel acessar o site")
            return False
        except Exception as e:
            self.critical_issues.append(f"‚ùå Erro inesperado: {str(e)}")
            return False
    
    def analyze_https(self):
        """Verifica se o site usa HTTPS"""
        if not self.url.startswith('https://'):
            self.critical_issues.append(
                "üîí Site n√£o usa HTTPS - Essencial para seguran√ßa e ranking no Google"
            )
        else:
            print("‚úÖ HTTPS ativo")
    
    def analyze_title(self):
        """Analisa a tag title"""
        title_tag = self.soup.find('title')
        
        if not title_tag:
            self.critical_issues.append("‚ùå Tag <title> n√£o encontrada")
            return
        
        title = title_tag.get_text().strip()
        title_length = len(title)
        keyword_in_title = self.keyword in title.lower()
        
        print(f"\nüìÑ TITLE: {title}")
        print(f"   Tamanho: {title_length} caracteres")
        
        if title_length < 50:
            self.opportunities.append(
                f"‚ö†Ô∏è Title muito curto ({title_length} chars) - Recomendado: 50-60 caracteres"
            )
        elif title_length > 60:
            self.opportunities.append(
                f"‚ö†Ô∏è Title muito longo ({title_length} chars) - Pode ser cortado no Google"
            )
        else:
            print("   ‚úÖ Tamanho ideal")
        
        if not keyword_in_title:
            self.opportunities.append(
                f"‚ö†Ô∏è Palavra-chave '{self.keyword}' n√£o encontrada no Title"
            )
        else:
            print(f"   ‚úÖ Palavra-chave '{self.keyword}' presente")
    
    def analyze_meta_description(self):
        """Analisa a meta description"""
        meta_desc = self.soup.find('meta', attrs={'name': 'description'})
        
        if not meta_desc or not meta_desc.get('content'):
            self.critical_issues.append("‚ùå Meta Description n√£o encontrada")
            return
        
        description = meta_desc.get('content').strip()
        desc_length = len(description)
        keyword_in_desc = self.keyword in description.lower()
        
        print(f"\nüìù META DESCRIPTION: {description[:100]}...")
        print(f"   Tamanho: {desc_length} caracteres")
        
        if desc_length < 150:
            self.opportunities.append(
                f"‚ö†Ô∏è Meta Description curta ({desc_length} chars) - Recomendado: 150-160"
            )
        elif desc_length > 160:
            self.opportunities.append(
                f"‚ö†Ô∏è Meta Description longa ({desc_length} chars) - Ser√° cortada no Google"
            )
        else:
            print("   ‚úÖ Tamanho ideal")
        
        if not keyword_in_desc:
            self.opportunities.append(
                f"‚ö†Ô∏è Palavra-chave '{self.keyword}' n√£o encontrada na Meta Description"
            )
        else:
            print(f"   ‚úÖ Palavra-chave '{self.keyword}' presente")
    
    def analyze_headings(self):
        """Analisa hierarquia de headings (H1-H6)"""
        print("\nüìë AN√ÅLISE DE HEADINGS:")
        
        h1_tags = self.soup.find_all('h1')
        h1_count = len(h1_tags)
        
        if h1_count == 0:
            self.critical_issues.append("‚ùå Nenhuma tag H1 encontrada")
        elif h1_count > 1:
            self.opportunities.append(
                f"‚ö†Ô∏è M√∫ltiplas tags H1 ({h1_count}) - Recomendado: apenas 1 por p√°gina"
            )
        else:
            h1_text = h1_tags[0].get_text().strip()
            print(f"   H1: {h1_text}")
            
            if self.keyword not in h1_text.lower():
                self.opportunities.append(
                    f"‚ö†Ô∏è Palavra-chave '{self.keyword}' n√£o encontrada no H1"
                )
            else:
                print(f"   ‚úÖ Palavra-chave '{self.keyword}' presente no H1")
        
        # An√°lise de H2 e H3
        h2_count = len(self.soup.find_all('h2'))
        h3_count = len(self.soup.find_all('h3'))
        
        print(f"   H2: {h2_count} encontradas")
        print(f"   H3: {h3_count} encontradas")
        
        if h2_count == 0:
            self.opportunities.append("‚ö†Ô∏è Nenhuma tag H2 - Use para estruturar o conte√∫do")
    
    def analyze_url_slug(self):
        """Analisa a URL/slug"""
        parsed_url = urlparse(self.url)
        slug = parsed_url.path
        
        print(f"\nüîó URL SLUG: {slug}")
        
        if self.keyword.replace(' ', '-') in slug or self.keyword.replace(' ', '_') in slug:
            print(f"   ‚úÖ Palavra-chave '{self.keyword}' presente na URL")
        else:
            self.opportunities.append(
                f"‚ö†Ô∏è Palavra-chave '{self.keyword}' n√£o encontrada na URL"
            )
        
        if len(slug) > 75:
            self.opportunities.append(
                f"‚ö†Ô∏è URL muito longa ({len(slug)} chars) - Prefira URLs curtas e descritivas"
            )
    
    def analyze_images(self):
        """Analisa imagens e atributos alt"""
        images = self.soup.find_all('img')
        total_images = len(images)
        images_without_alt = 0
        images_without_dimensions = 0
        images_with_keyword = 0
        
        print(f"\nüñºÔ∏è AN√ÅLISE DE IMAGENS: {total_images} encontradas")
        
        for img in images:
            alt = img.get('alt', '').strip()
            
            if not alt:
                images_without_alt += 1
            elif self.keyword in alt.lower():
                images_with_keyword += 1
            
            if not img.get('width') or not img.get('height'):
                images_without_dimensions += 1
        
        if images_without_alt > 0:
            self.opportunities.append(
                f"‚ö†Ô∏è {images_without_alt} imagens sem atributo ALT - Importante para acessibilidade e SEO"
            )
        
        if images_without_dimensions > 0:
            self.opportunities.append(
                f"‚ö†Ô∏è {images_without_dimensions} imagens sem width/height - Causa Layout Shift (CLS)"
            )
        
        if total_images > 0 and images_with_keyword == 0:
            self.opportunities.append(
                f"‚ö†Ô∏è Nenhuma imagem com palavra-chave '{self.keyword}' no ALT"
            )
        
        print(f"   Sem ALT: {images_without_alt}")
        print(f"   Sem dimens√µes: {images_without_dimensions}")
        print(f"   Com palavra-chave: {images_with_keyword}")
    
    def analyze_content(self):
        """Analisa o conte√∫do da p√°gina"""
        # Remove scripts e styles
        for script in self.soup(['script', 'style', 'nav', 'footer', 'header']):
            script.decompose()
        
        body = self.soup.find('body')
        if not body:
            self.critical_issues.append("‚ùå Tag <body> n√£o encontrada")
            return
        
        text = body.get_text()
        words = re.findall(r'\b\w+\b', text.lower())
        word_count = len(words)
        
        print(f"\nüìä AN√ÅLISE DE CONTE√öDO:")
        print(f"   Total de palavras: {word_count}")
        
        if word_count < 300:
            self.critical_issues.append(
                f"‚ùå Conte√∫do Raso (Thin Content) - Apenas {word_count} palavras. Recomendado: m√≠nimo 300"
            )
        elif word_count < 600:
            self.opportunities.append(
                f"‚ö†Ô∏è Conte√∫do curto ({word_count} palavras) - Considere expandir para 800-1500 palavras"
            )
        else:
            print("   ‚úÖ Conte√∫do substancial")
        
        # Densidade da palavra-chave
        keyword_count = text.lower().count(self.keyword)
        keyword_density = (keyword_count / word_count * 100) if word_count > 0 else 0
        
        print(f"   Palavra-chave '{self.keyword}': {keyword_count} ocorr√™ncias")
        print(f"   Densidade: {keyword_density:.2f}%")
        
        if keyword_density < 0.5:
            self.opportunities.append(
                f"‚ö†Ô∏è Densidade da palavra-chave muito baixa ({keyword_density:.2f}%) - Recomendado: 1-2%"
            )
        elif keyword_density > 3:
            self.opportunities.append(
                f"‚ö†Ô∏è Densidade da palavra-chave muito alta ({keyword_density:.2f}%) - Risco de keyword stuffing"
            )
        
        # Verifica formata√ß√£o
        strong_tags = len(self.soup.find_all(['strong', 'b']))
        lists = len(self.soup.find_all(['ul', 'ol']))
        
        print(f"   Tags <strong>/<b>: {strong_tags}")
        print(f"   Listas <ul>/<ol>: {lists}")
        
        if strong_tags == 0:
            self.opportunities.append("‚ö†Ô∏è Nenhum texto em negrito - Use para destacar termos importantes")
        
        if lists == 0:
            self.opportunities.append("‚ö†Ô∏è Nenhuma lista - Listas melhoram a legibilidade")
    
    def analyze_links(self):
        """Analisa links internos e externos"""
        links = self.soup.find_all('a', href=True)
        internal_links = []
        external_links = []
        broken_links = []
        links_without_title = 0
        
        print(f"\nüîó AN√ÅLISE DE LINKS: {len(links)} encontrados")
        
        for link in links:
            href = link.get('href', '')
            
            if not href or href.startswith('#') or href.startswith('javascript:'):
                continue
            
            # Converte para URL absoluta
            absolute_url = urljoin(self.url, href)
            parsed = urlparse(absolute_url)
            
            # Classifica interno vs externo
            if parsed.netloc == self.domain or not parsed.netloc:
                internal_links.append(absolute_url)
            else:
                external_links.append(absolute_url)
            
            # Verifica atributo title
            if not link.get('title'):
                links_without_title += 1
        
        print(f"   Links internos: {len(internal_links)}")
        print(f"   Links externos: {len(external_links)}")
        print(f"   Sem atributo title: {links_without_title}")
        
        if len(internal_links) < 3:
            self.opportunities.append(
                "‚ö†Ô∏è Poucos links internos - Melhore a linkagem interna para distribuir autoridade"
            )
        
        # Verifica links quebrados (apenas uma amostra para n√£o demorar muito)
        print("\n   Verificando links quebrados (amostra)...")
        sample_links = (internal_links + external_links)[:10]
        
        for link in sample_links:
            try:
                response = requests.head(link, headers=self.headers, timeout=3, allow_redirects=True)
                if response.status_code >= 400:
                    broken_links.append((link, response.status_code))
            except:
                pass
        
        if broken_links:
            self.critical_issues.append(
                f"‚ùå {len(broken_links)} links quebrados encontrados na amostra"
            )
            for link, status in broken_links[:3]:
                print(f"      - {link} (Status: {status})")
    
    def analyze_mobile_viewport(self):
        """Verifica meta viewport para mobile"""
        viewport = self.soup.find('meta', attrs={'name': 'viewport'})
        
        if not viewport:
            self.critical_issues.append(
                "‚ùå Meta tag viewport n√£o encontrada - Essencial para responsividade mobile"
            )
        else:
            print("\nüì± ‚úÖ Meta viewport presente (Mobile-friendly)")
    
    def analyze_robots_sitemap(self):
        """Verifica robots.txt e sitemap.xml"""
        base_url = f"{urlparse(self.url).scheme}://{self.domain}"
        
        print("\nü§ñ VERIFICANDO ARQUIVOS T√âCNICOS:")
        
        # Verifica robots.txt
        try:
            robots_response = requests.get(f"{base_url}/robots.txt", headers=self.headers, timeout=5)
            if robots_response.status_code == 200:
                print("   ‚úÖ robots.txt encontrado")
            else:
                self.opportunities.append("‚ö†Ô∏è robots.txt n√£o encontrado - Recomendado para controlar crawlers")
        except:
            self.opportunities.append("‚ö†Ô∏è robots.txt n√£o acess√≠vel")
        
        # Verifica sitemap.xml
        try:
            sitemap_response = requests.get(f"{base_url}/sitemap.xml", headers=self.headers, timeout=5)
            if sitemap_response.status_code == 200:
                print("   ‚úÖ sitemap.xml encontrado")
            else:
                self.opportunities.append("‚ö†Ô∏è sitemap.xml n√£o encontrado - Essencial para indexa√ß√£o")
        except:
            self.opportunities.append("‚ö†Ô∏è sitemap.xml n√£o acess√≠vel")
    
    def analyze_schema_markup(self):
        """Verifica presen√ßa de Schema Markup (JSON-LD)"""
        schema_scripts = self.soup.find_all('script', type='application/ld+json')
        
        print(f"\nüìã SCHEMA MARKUP (Dados Estruturados):")
        
        if schema_scripts:
            print(f"   ‚úÖ {len(schema_scripts)} schema(s) encontrado(s)")
            for i, script in enumerate(schema_scripts[:3], 1):
                try:
                    import json
                    data = json.loads(script.string)
                    schema_type = data.get('@type', 'Unknown')
                    print(f"      {i}. Tipo: {schema_type}")
                except:
                    pass
        else:
            self.opportunities.append(
                "‚ö†Ô∏è Nenhum Schema Markup encontrado - Use dados estruturados para Rich Snippets"
            )
    
    def analyze_performance(self):
        """Analisa performance"""
        print(f"\n‚ö° PERFORMANCE:")
        print(f"   TTFB (Time to First Byte): {self.ttfb:.3f}s")
        
        if self.ttfb > 0.5:
            self.critical_issues.append(
                f"‚ùå TTFB muito alto ({self.ttfb:.3f}s) - Servidor lento. Recomendado: < 0.5s"
            )
        elif self.ttfb > 0.3:
            self.opportunities.append(
                f"‚ö†Ô∏è TTFB moderado ({self.ttfb:.3f}s) - Considere otimizar servidor"
            )
        else:
            print("   ‚úÖ TTFB excelente")
        
        self.performance_data['ttfb'] = self.ttfb
        self.performance_data['page_size'] = len(self.response.content) / 1024  # KB
        
        print(f"   Tamanho da p√°gina: {self.performance_data['page_size']:.2f} KB")
        
        if self.performance_data['page_size'] > 1024:  # > 1MB
            self.opportunities.append(
                f"‚ö†Ô∏è P√°gina muito pesada ({self.performance_data['page_size']:.2f} KB) - Otimize imagens e c√≥digo"
            )
    
    def generate_action_plan(self):
        """Gera plano de a√ß√£o baseado nas an√°lises"""
        self.action_plan = [
            "\n" + "="*80,
            "üìã PLANO DE A√á√ÉO - PRIORIDADES PARA RANKEAR NO GOOGLE",
            "="*80,
            "\nüî¥ CR√çTICO (Fa√ßa IMEDIATAMENTE):"
        ]
        
        if self.critical_issues:
            for issue in self.critical_issues:
                self.action_plan.append(f"   {issue}")
        else:
            self.action_plan.append("   ‚úÖ Nenhum problema cr√≠tico encontrado!")
        
        self.action_plan.extend([
            "\nüü° OPORTUNIDADES DE OTIMIZA√á√ÉO:",
        ])
        
        if self.opportunities:
            for opp in self.opportunities:
                self.action_plan.append(f"   {opp}")
        else:
            self.action_plan.append("   ‚úÖ P√°gina bem otimizada!")
        
        self.action_plan.extend([
            "\nüîµ ESTRAT√âGIA DE LINK BUILDING:",
            "   1. Crie conte√∫do de alta qualidade e compartilh√°vel",
            "   2. Guest posts em blogs relevantes do seu nicho",
            "   3. Parcerias com influenciadores e sites de autoridade",
            "   4. Divulgue em redes sociais e f√≥runs especializados",
            "   5. Crie infogr√°ficos e recursos link√°veis",
            "   6. Participe de comunidades online (Reddit, Quora, etc)",
            "   7. Monitore men√ß√µes √† sua marca e pe√ßa links",
            "\nüéØ PR√ìXIMOS PASSOS:",
            "   1. Corrija todos os problemas cr√≠ticos listados acima",
            "   2. Implemente as oportunidades de otimiza√ß√£o",
            "   3. Crie conte√∫do adicional focado na palavra-chave",
            "   4. Monitore o ranking semanalmente (Google Search Console)",
            "   5. Construa backlinks de qualidade gradualmente",
            "   6. Atualize o conte√∫do regularmente (Google valoriza frescor)",
        ])
    
    def generate_report(self):
        """Gera relat√≥rio completo"""
        report = [
            "\n" + "="*80,
            "üîç RELAT√ìRIO DE AUDITORIA SEO",
            "="*80,
            f"URL: {self.url}",
            f"Palavra-chave foco: {self.keyword}",
            f"Data: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}",
            "="*80,
        ]
        
        # Adiciona plano de a√ß√£o
        report.extend(self.action_plan)
        
        report.extend([
            "\n" + "="*80,
            "üìä RESUMO DA AN√ÅLISE",
            "="*80,
            f"‚úÖ Problemas Cr√≠ticos: {len(self.critical_issues)}",
            f"‚ö†Ô∏è  Oportunidades: {len(self.opportunities)}",
            f"‚ö° TTFB: {self.ttfb:.3f}s",
            f"üì¶ Tamanho: {self.performance_data.get('page_size', 0):.2f} KB",
            "="*80,
        ])
        
        return "\n".join(report)
    
    def run_audit(self):
        """Executa auditoria completa"""
        print("\n" + "="*80)
        print("üöÄ INICIANDO AUDITORIA SEO COMPLETA")
        print("="*80)
        
        if not self.fetch_page():
            print("\n‚ùå N√£o foi poss√≠vel acessar a p√°gina. Auditoria interrompida.")
            return
        
        # Executa todas as an√°lises
        self.analyze_https()
        self.analyze_title()
        self.analyze_meta_description()
        self.analyze_headings()
        self.analyze_url_slug()
        self.analyze_images()
        self.analyze_content()
        self.analyze_links()
        self.analyze_mobile_viewport()
        self.analyze_robots_sitemap()
        self.analyze_schema_markup()
        self.analyze_performance()
        
        # Gera plano de a√ß√£o
        self.generate_action_plan()
        
        # Gera e exibe relat√≥rio
        report = self.generate_report()
        print(report)
        
        # Salva relat√≥rio em arquivo
        filename = f"seo_audit_{self.domain.replace('.', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nüíæ Relat√≥rio salvo em: {filename}")
        print("\n‚úÖ Auditoria conclu√≠da!")


def main():
    """Fun√ß√£o principal"""
    print("\n" + "="*80)
    print("üéØ FERRAMENTA DE AUDITORIA SEO - ESPECIALISTA EM RANKEAMENTO GOOGLE")
    print("="*80)
    
    # Solicita dados do usu√°rio
    url = input("\nüìç Digite a URL completa para an√°lise (ex: https://exemplo.com): ").strip()
    keyword = input("üîë Digite a palavra-chave foco (ex: jogo do impostor): ").strip()
    
    if not url or not keyword:
        print("\n‚ùå URL e palavra-chave s√£o obrigat√≥rios!")
        return
    
    # Valida URL
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    # Executa auditoria
    auditor = SEOAuditor(url, keyword)
    auditor.run_audit()


if __name__ == "__main__":
    main()
